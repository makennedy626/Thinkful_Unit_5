{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Return specific pieces of information (rather than just downloading a whole page)\n",
    "2) Iterate over multiple pages/queries\n",
    "3) Save the data to your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import json\n",
    "import requests\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "#Build a crawler to crawl Google's top news articles and pull articles containing 'bitcoin'\n",
    "class GoogleSpider(scrapy.Spider):\n",
    "    name = \"GS\"\n",
    "    \n",
    "    allowed_domains = ['newsapi.org']\n",
    "    \n",
    "    # Here is where we insert our API call to get Google news articles related to bitcoin.\n",
    "    start_urls = ['https://newsapi.org/v2/everything?q=bitcoin&apiKey=2613ce5e838a464b814b7d5b4c2e6bf8'\n",
    "                 ,'https://newsapi.org/v2/everything?q=ethereum&apiKey=2613ce5e838a464b814b7d5b4c2e6bf8']\n",
    "      \n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        data = json.loads(response.body_as_unicode())\n",
    "        data2 = []\n",
    "        for article in data['articles']:\n",
    "            yield {\n",
    "                'url' : article['url']\n",
    "            }\n",
    "                \n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'GoogleLinks5.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'MatthewGoogleNewsCrawler (makennedy626@gmail.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 10 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(GoogleSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
