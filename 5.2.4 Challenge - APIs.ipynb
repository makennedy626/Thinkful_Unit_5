{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Return specific pieces of information (rather than just downloading a whole page)\n",
    "2) Iterate over multiple pages/queries\n",
    "3) Save the data to your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first test\n",
      "second test\n",
      "<200 https://newsapi.org/v2/everything?q=bitcoin&apiKey=2613ce5e838a464b814b7d5b4c2e6bf8>\n",
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import json\n",
    "\n",
    "#Build a crawler to crawl Google's top news articles and pull articles containing 'bitcoin'\n",
    "class GoogleSpider(scrapy.Spider):\n",
    "    name = \"GS\"\n",
    "    print('first test')\n",
    "\n",
    "    \n",
    "    allowed_domains = ['newsapi.org']\n",
    "    \n",
    "    # Here is where we insert our API call to get Google news articles related to bitcoin.\n",
    "    start_urls = ['https://newsapi.org/v2/everything?q=bitcoin&apiKey=2613ce5e838a464b814b7d5b4c2e6bf8']\n",
    "    print('second test')\n",
    "\n",
    "      \n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        print(response)\n",
    "        for h3 in response.xpath('//h3').extract():\n",
    "            print('success!')\n",
    "            \n",
    "        for url in response.xpath('//a/@href').extract():\n",
    "            yield scrapy.Request(url, callback = self.parse)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        for item in response.xpath('//lh'):\n",
    "            print(response)\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "'''\n",
    "        print(response)\n",
    "        links = json.loads(response.body_as_unicode())\n",
    "        links.extract()\n",
    "        for index, link in enumerate(links):\n",
    "            # within the json response, look inside the article dictionary, and pull the url and title\n",
    "            args = (index, link.xpath('@href').extract(), link.xpath('img/@src').extract())\n",
    "            print('Link number %d points to url %s and title %s' % args)\n",
    "    \n",
    "    # Iterate over multiple pages\n",
    "    \n",
    "     # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "    '''\n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'GoogleLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'MatthewGoogleNewsCrawler (makennedy626@gmail.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 10 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(GoogleSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3a904d211b9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Checking whether we got data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleLinks.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'records'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[0;32m    352\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[0;32m    353\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                           date_unit).parse()\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 652\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "data=pd.read_json('GoogleLinks.json', orient='records')\n",
    "print(data.shape)\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "googlefinance.client is a python client library for google finance api\n",
    "\n",
    "Installation\n",
    "$ pip install googlefinance.client\n",
    "Usage\n",
    "from googlefinance.client import get_price_data, get_prices_data, get_prices_time_data\n",
    "\n",
    "# Dow Jones\n",
    "param = {\n",
    "    'q': \".DJI\", # Stock symbol (ex: \"AAPL\")\n",
    "    'i': \"86400\", # Interval size in seconds (\"86400\" = 1 day intervals)\n",
    "    'x': \"INDEXDJX\", # Stock exchange symbol on which stock is traded (ex: \"NASD\")\n",
    "    'p': \"1Y\" # Period (Ex: \"1Y\" = 1 year)\n",
    "}\n",
    "# get price data (return pandas dataframe)\n",
    "df = get_price_data(param)\n",
    "print(df)\n",
    "#                          Open      High       Low     Close     Volume\n",
    "# 2016-05-17 05:00:00  17531.76   17755.8  17531.76  17710.71   88436105\n",
    "# 2016-05-18 05:00:00  17701.46  17701.46  17469.92  17529.98  103253947\n",
    "# 2016-05-19 05:00:00  17501.28  17636.22  17418.21  17526.62   79038923\n",
    "# 2016-05-20 05:00:00  17514.16  17514.16  17331.07   17435.4   95531058\n",
    "# 2016-05-21 05:00:00  17437.32  17571.75  17437.32  17500.94  111992332\n",
    "# ...                       ...       ...       ...       ...        ...\n",
    "\n",
    "params = [\n",
    "    # Dow Jones\n",
    "    {\n",
    "        'q': \".DJI\",\n",
    "        'x': \"INDEXDJX\",\n",
    "    },\n",
    "    # NYSE COMPOSITE (DJ)\n",
    "    {\n",
    "        'q': \"NYA\",\n",
    "        'x': \"INDEXNYSEGIS\",\n",
    "    },\n",
    "    # S&P 500\n",
    "    {\n",
    "        'q': \".INX\",\n",
    "        'x': \"INDEXSP\",\n",
    "    }\n",
    "]\n",
    "period = \"1Y\"\n",
    "# get open, high, low, close, volume data (return pandas dataframe)\n",
    "df = get_prices_data(params, period)\n",
    "print(df)\n",
    "#            .DJI_Open  .DJI_High  .DJI_Low  .DJI_Close  .DJI_Volume  \\\n",
    "# 2016-07-20   18503.12   18562.53  18495.11    18559.01    85840786\n",
    "# 2016-07-21   18582.70   18622.01  18555.65    18595.03    93233337\n",
    "# 2016-07-22   18589.96   18590.44  18469.67    18517.23    86803016\n",
    "# 2016-07-23   18524.15   18571.30  18491.59    18570.85    87706622\n",
    "# 2016-07-26   18554.49   18555.69  18452.62    18493.06    76807470\n",
    "# ...               ...        ...       ...         ...         ...\n",
    "\n",
    "params = [\n",
    "    # Dow Jones\n",
    "    {\n",
    "        'q': \".DJI\",\n",
    "        'x': \"INDEXDJX\",\n",
    "    },\n",
    "    # NYSE COMPOSITE (DJ)\n",
    "    {\n",
    "        'q': \"NYA\",\n",
    "        'x': \"INDEXNYSEGIS\",\n",
    "    },\n",
    "    # S&P 500\n",
    "    {\n",
    "        'q': \".INX\",\n",
    "        'x': \"INDEXSP\",\n",
    "    }\n",
    "]\n",
    "period = \"1Y\"\n",
    "interval = 60*30 # 30 minutes\n",
    "# get open, high, low, close, volume time data (return pandas dataframe)\n",
    "df = get_prices_time_data(params, period, interval)\n",
    "print(df)\n",
    "#                      .DJI_Open  .DJI_High  .DJI_Low  .DJI_Close  .DJI_Volume  \\\n",
    "# 2016-07-19 23:00:00   18503.12   18542.13  18495.11    18522.47            0\n",
    "# 2016-07-19 23:30:00   18522.44   18553.30  18509.25    18546.27            0\n",
    "# 2016-07-20 00:00:00   18546.20   18549.59  18519.77    18539.93            0\n",
    "# 2016-07-20 00:30:00   18540.24   18549.80  18526.99    18534.18            0\n",
    "# 2016-07-20 01:00:00   18534.05   18540.38  18507.34    18516.41            0\n",
    "# ...  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
